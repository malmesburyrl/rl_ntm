{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IPython magic to reload libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Extra code so Juypter doesn't crash on mac os\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'utils')  # noqa\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import ntm\n",
    "import interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "train_iter = 100\n",
    "batch_size = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(2), Discrete(2), Discrete(5))\n",
      "Discrete(6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zachyamaoka/opt/anaconda3/envs/tf2_robo_research/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# Task\n",
    "env = gym.make('Copy-v0')\n",
    "env = env.unwrapped\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "tape_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "memory_input (InputLayer)       [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 11)           0           tape_input[0][0]                 \n",
      "                                                                 memory_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 32)           384         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           1056        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_content (Dense)          (None, 5)            165         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_bool (Dense)             (None, 2)            66          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_head_control (Dense)      (None, 2)            66          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "memory_head_control (Dense)     (None, 2)            66          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "memory_content (Dense)          (None, 10)           330         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,133\n",
      "Trainable params: 2,133\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Controller \n",
    "Controller = ntm.NTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 1)\n",
      "Loss:  [1.4307102]\n"
     ]
    }
   ],
   "source": [
    "#Over Fit on single char\n",
    "output = Controller.run(tape_input=1)\n",
    "print(output)\n",
    "Controller.update(output_target=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy - obs: C action: A, yes, right\n",
      "Loss:  [1.1708977]\n",
      "Policy - obs: E action: C, yes, right\n",
      "Loss:  [1.4809636]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [2.01573]\n",
      "Policy - obs: D action: C, yes, right\n",
      "Loss:  [2.041598]\n",
      "Policy - obs: E action: C, yes, right\n",
      "Loss:  [1.4207302]\n",
      "Policy - obs: D action: A, yes, right\n",
      "Loss:  [1.8481375]\n",
      "Policy - obs: B action: B, yes, right\n",
      "Loss:  [2.0037434]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.7003218]\n",
      "Policy - obs: D action: C, yes, right\n",
      "Loss:  [1.6063377]\n",
      "Policy - obs: C action: C, yes, right\n",
      "Loss:  [1.3801754]\n",
      "Policy - obs: B action: D, yes, right\n",
      "Loss:  [1.872267]\n",
      "Policy - obs: D action: E, yes, right\n",
      "Loss:  [1.5729563]\n",
      "Policy - obs: A action: D, yes, right\n",
      "Loss:  [1.8047199]\n",
      "Policy - obs: E action: C, yes, right\n",
      "Loss:  [1.5008891]\n",
      "Policy - obs: D action: B, yes, right\n",
      "Loss:  [1.5327643]\n",
      "Policy - obs: B action: E, yes, right\n",
      "Loss:  [1.8022217]\n",
      "Policy - obs: C action: E, yes, right\n",
      "Loss:  [1.4254212]\n",
      "Policy - obs: B action: E, yes, right\n",
      "Loss:  [1.7260494]\n",
      "Policy - obs: C action: C, yes, right\n",
      "Loss:  [1.3728951]\n",
      "Policy - obs: A action: E, yes, right\n",
      "Loss:  [1.7855974]\n",
      "Policy - obs: E action: A, yes, right\n",
      "Loss:  [1.4750944]\n",
      "Policy - obs: C action: B, yes, right\n",
      "Loss:  [1.3365248]\n",
      "Policy - obs: A action: E, yes, right\n",
      "Loss:  [1.7103004]\n",
      "Policy - obs: D action: B, yes, right\n",
      "Loss:  [1.5881959]\n",
      "Policy - obs: E action: D, yes, right\n",
      "Loss:  [1.3987767]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [1.6245167]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.5330604]\n",
      "Policy - obs: B action: A, yes, right\n",
      "Loss:  [1.7629646]\n",
      "Policy - obs: A action: E, yes, right\n",
      "Loss:  [1.5802032]\n",
      "Policy - obs: C action: B, yes, right\n",
      "Loss:  [1.4179484]\n",
      "Policy - obs: A action: E, yes, right\n",
      "Loss:  [1.5123234]\n",
      "Policy - obs: A action: E, yes, right\n",
      "Loss:  [1.44506]\n",
      "Policy - obs: C action: D, yes, right\n",
      "Loss:  [1.3397864]\n",
      "Policy - obs: D action: C, yes, right\n",
      "Loss:  [1.528153]\n",
      "Policy - obs: A action: B, yes, right\n",
      "Loss:  [1.400555]\n",
      "Policy - obs: A action: D, yes, right\n",
      "Loss:  [1.3355889]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.4168133]\n",
      "Policy - obs: C action: A, yes, right\n",
      "Loss:  [1.3591303]\n",
      "Policy - obs: C action: D, yes, right\n",
      "Loss:  [1.2870458]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [1.2962973]\n",
      "Policy - obs: A action: C, yes, right\n",
      "Loss:  [1.2368084]\n",
      "Policy - obs: C action: E, yes, right\n",
      "Loss:  [1.2600489]\n",
      "Policy - obs: C action: C, yes, right\n",
      "Loss:  [1.1826569]\n",
      "Policy - obs: D action: C, yes, right\n",
      "Loss:  [1.5233041]\n",
      "Policy - obs: A action: C, yes, right\n",
      "Loss:  [1.2167851]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.3783514]\n",
      "Policy - obs: A action: D, yes, right\n",
      "Loss:  [1.1684756]\n",
      "Policy - obs: D action: B, yes, right\n",
      "Loss:  [1.2948637]\n",
      "Policy - obs: C action: E, yes, right\n",
      "Loss:  [1.2978513]\n",
      "Policy - obs: B action: E, yes, right\n",
      "Loss:  [2.0379229]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.2615087]\n",
      "Policy - obs: E action: C, yes, right\n",
      "Loss:  [1.7147415]\n",
      "Policy - obs: D action: E, yes, right\n",
      "Loss:  [1.2194501]\n",
      "Policy - obs: C action: C, yes, right\n",
      "Loss:  [1.3529121]\n",
      "Policy - obs: C action: C, yes, right\n",
      "Loss:  [1.2697212]\n",
      "Policy - obs: E action: C, yes, right\n",
      "Loss:  [1.5881287]\n",
      "Policy - obs: C action: E, yes, right\n",
      "Loss:  [1.2068256]\n",
      "Policy - obs: A action: B, yes, right\n",
      "Loss:  [1.2105733]\n",
      "Policy - obs: E action: E, yes, right\n",
      "Loss:  [1.4263848]\n",
      "Policy - obs: C action: B, yes, right\n",
      "Loss:  [1.1715027]\n",
      "Policy - obs: C action: C, yes, right\n",
      "Loss:  [1.0936455]\n",
      "Policy - obs: C action: D, yes, right\n",
      "Loss:  [1.0221422]\n",
      "Policy - obs: C action: C, yes, right\n",
      "Loss:  [0.960138]\n",
      "Policy - obs: E action: D, yes, right\n",
      "Loss:  [1.3561602]\n",
      "Policy - obs: B action: A, yes, right\n",
      "Loss:  [2.231174]\n",
      "Policy - obs: A action: C, yes, right\n",
      "Loss:  [1.2465446]\n",
      "Policy - obs: B action: C, yes, right\n",
      "Loss:  [2.0528936]\n",
      "Policy - obs: E action: C, yes, right\n",
      "Loss:  [1.2117051]\n",
      "Policy - obs: C action: C, yes, right\n",
      "Loss:  [1.103287]\n",
      "Policy - obs: B action: D, yes, right\n",
      "Loss:  [1.9550714]\n",
      "Policy - obs: A action: B, yes, right\n",
      "Loss:  [1.2192882]\n",
      "Policy - obs: A action: B, yes, right\n",
      "Loss:  [1.1476831]\n",
      "Policy - obs: C action: C, yes, right\n",
      "Loss:  [1.1021264]\n",
      "Policy - obs: A action: D, yes, right\n",
      "Loss:  [1.121848]\n",
      "Policy - obs: C action: E, yes, right\n",
      "Loss:  [1.0245706]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.8928609]\n",
      "Policy - obs: D action: A, yes, right\n",
      "Loss:  [1.6360021]\n",
      "Policy - obs: B action: C, yes, right\n",
      "Loss:  [1.8678272]\n",
      "Policy - obs: C action: D, yes, right\n",
      "Loss:  [1.1299504]\n",
      "Policy - obs: A action: C, yes, right\n",
      "Loss:  [1.1128465]\n",
      "Policy - obs: B action: B, yes, right\n",
      "Loss:  [1.7730803]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [1.0646679]\n",
      "Policy - obs: E action: E, yes, right\n",
      "Loss:  [1.4389087]\n",
      "Policy - obs: C action: E, yes, right\n",
      "Loss:  [1.1376392]\n",
      "Policy - obs: E action: C, yes, right\n",
      "Loss:  [1.2433665]\n",
      "Policy - obs: C action: A, yes, right\n",
      "Loss:  [1.084906]\n",
      "Policy - obs: D action: A, yes, right\n",
      "Loss:  [1.7237041]\n",
      "Policy - obs: A action: C, yes, right\n",
      "Loss:  [1.070091]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [0.9974968]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.4988948]\n",
      "Policy - obs: E action: B, yes, right\n",
      "Loss:  [1.2507983]\n",
      "Policy - obs: E action: E, yes, right\n",
      "Loss:  [1.069763]\n",
      "Policy - obs: E action: E, yes, right\n",
      "Loss:  [0.9211959]\n",
      "Policy - obs: E action: E, yes, right\n",
      "Loss:  [0.79014766]\n",
      "Policy - obs: B action: A, yes, right\n",
      "Loss:  [1.877535]\n",
      "Policy - obs: C action: D, yes, right\n",
      "Loss:  [1.3499368]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [1.0070221]\n",
      "Policy - obs: E action: E, yes, right\n",
      "Loss:  [0.74464226]\n",
      "Policy - obs: E action: D, yes, right\n",
      "Loss:  [0.6357675]\n",
      "Policy - obs: D action: E, yes, right\n",
      "Loss:  [1.864385]\n",
      "Policy - obs: B action: B, yes, right\n",
      "Loss:  [1.7793375]\n",
      "Policy - obs: D action: E, yes, right\n",
      "Loss:  [1.5714847]\n",
      "Policy - obs: A action: D, yes, right\n",
      "Loss:  [0.9933133]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [0.92214656]\n",
      "Policy - obs: A action: B, yes, right\n",
      "Loss:  [0.86242694]\n",
      "Policy - obs: A action: C, yes, right\n",
      "Loss:  [0.80760854]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [0.75739014]\n",
      "Policy - obs: D action: E, yes, right\n",
      "Loss:  [1.4431775]\n",
      "Policy - obs: C action: D, yes, right\n",
      "Loss:  [1.5241786]\n",
      "Policy - obs: C action: E, yes, right\n",
      "Loss:  [1.361829]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [0.7794858]\n",
      "Policy - obs: E action: D, yes, right\n",
      "Loss:  [0.95232785]\n",
      "Policy - obs: B action: A, yes, right\n",
      "Loss:  [1.8457574]\n",
      "Policy - obs: C action: A, yes, right\n",
      "Loss:  [1.3407702]\n",
      "Policy - obs: B action: C, yes, right\n",
      "Loss:  [1.743017]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.4835778]\n",
      "Policy - obs: E action: D, yes, right\n",
      "Loss:  [0.8953906]\n",
      "Policy - obs: D action: E, yes, right\n",
      "Loss:  [1.4081827]\n",
      "Policy - obs: C action: E, yes, right\n",
      "Loss:  [1.3805465]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.3189552]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.1920515]\n",
      "Policy - obs: C action: C, yes, right\n",
      "Loss:  [1.3594676]\n",
      "Policy - obs: D action: E, yes, right\n",
      "Loss:  [1.125288]\n",
      "Policy - obs: B action: A, yes, right\n",
      "Loss:  [1.75438]\n",
      "Policy - obs: C action: A, yes, right\n",
      "Loss:  [1.3533765]\n",
      "Policy - obs: C action: D, yes, right\n",
      "Loss:  [1.2478943]\n",
      "Policy - obs: D action: D, yes, right\n",
      "Loss:  [1.1376903]\n",
      "Policy - obs: C action: D, yes, right\n",
      "Loss:  [1.2073946]\n",
      "Policy - obs: E action: C, yes, right\n",
      "Loss:  [1.1853555]\n",
      "Policy - obs: A action: B, yes, right\n",
      "Loss:  [0.9641212]\n",
      "Policy - obs: C action: D, yes, right\n",
      "Loss:  [1.1640031]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [0.8819462]\n",
      "Policy - obs: A action: A, yes, right\n",
      "Loss:  [0.79065883]\n",
      "Policy - obs: D action: C, yes, right\n",
      "Loss:  [1.2731991]\n",
      "Policy - obs: C action: B, yes, right\n",
      "Loss:  [1.1646516]\n",
      "Policy - obs: A action: C, yes, right\n",
      "Loss:  [0.7485175]\n",
      "Policy - obs: E action: C, yes, right\n",
      "Loss:  [1.1541125]\n"
     ]
    }
   ],
   "source": [
    "output_control_cmds = [\"no\",\"yes\"]\n",
    "input_control_cmds = [\"left\",\"right\"]\n",
    "int_to_char = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
    "render_env = False\n",
    "for i in range(train_iter):\n",
    "# for i in range(1):\n",
    "\n",
    "    # Collect experince using current policy/controller\n",
    "    for j in range(batch_size):\n",
    "        \n",
    "        episode_steps = 0\n",
    "        observation = env.reset()\n",
    "        if render_env: env.render()\n",
    "\n",
    "        while True:\n",
    "            episode_steps += 1\n",
    "            \n",
    "            #Run RL-NTM controller and get outputs\n",
    "            outputs = Controller.run(tape_input=observation)\n",
    "\n",
    "            output_content = outputs[0] # Which character to write (ignored if the above sub-action is 0) \n",
    "            output_control = outputs[1] #  Whether to write to the output tape [\"no\",\"yes\"] = [0,1]\n",
    "            input_control = outputs[2] # Direction to move the read head [\"left\",\"right\"] = [0,1]\n",
    "           \n",
    "            actions = (input_control, output_control, output_content)\n",
    "            \n",
    "            print(\"Policy - obs: {} action: {}, {}, {}\".format(int_to_char[observation],\n",
    "                  int_to_char[output_content],\n",
    "                  output_control_cmds[output_control],\n",
    "                  input_control_cmds[input_control]))\n",
    "\n",
    "            #Get output target before updating enviroment\n",
    "            target = env.target[env.write_head_position]\n",
    "            observation, reward, done, info = env.step(actions)\n",
    "            if render_env: env.render()\n",
    "                \n",
    "            if output_control_cmds[output_control] == \"yes\":\n",
    "                Controller.update(output_target=target)\n",
    "                \n",
    "            if done:\n",
    "                Controller.backup()\n",
    "                done = False\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
